# Задание 9. Алгоритмы на деревьях

Требуется в Jupyter выполнить следующие задачи. Выполненное задание сдаётся в виде файла с названием task_9.ipynb в репозитории c остальными заданиями практикума.

1. Загрузите датасет при помощи `sklearn.datasets.fetch_covtype`. Данная выборка довольно большая, поэтому можно взять первые 50,000 семплов. Не забудьте при этом сначала перемешать выборку, в `fetch_covtype` можно для этого передать `shuffle=True`. В этом задании предлагается задача мультиклассификации на 7 классов, все объекты имеют 54 признака. Подробнее про выборку можно почитать здесь: https://archive.ics.uci.edu/ml/datasets/Covertype
2. Стратифицированно по классам разделите датасет на обучающую и тестовую выборку, например, в отношении 1 к 4. Можно использовать `sklearn.model_selection.train_test_split`.
3. Обучите решающее дерево (`sklearn.tree.DecisionTreeClassifier`). Какая точность предсказания (`sklearn.metrics.accuracy_score`) на тестовой выборке получилась? Подберите гиперпараметры алгоритма  `ccp_alpha`, `max_depth`, `min_samples_split`, `min_samples_leaf`, обеспечивающие наилучшую точность. Для перебора гиперпараметров удобно использовать `sklearn.model_selection.GridSearchCV`.
4. Аналогичным образом обучите случайный лес (`sklearn.ensemble.RandomForestClassifier`), подберите оптимальные гиперпараметры и сравните точность с решающим деревом.
5. Установите CatBoost, например, при помощи команды `pip install catboost`. Документацию можно почитать здесь: https://catboost.ai/docs/concepts/python-quickstart.html
6. Обучите на тех же данных CatBoostClassifier. Имеет смысл подобрать оптимальные параметры `iterations` (число итераций, т.е. деревьев), `learning_rate` (скорость обучения), `depth` (максимальная глубина каждого дерева). Для подбора числа итераций также можно использовать `use_best_model`, для этого на вход CatBoost'у нужно подать валидационную выборку (параметр `eval_set` в `fit`). Тогда CatBoost вернёт модель, которая наиболее оптимальна по функции потерь или метрике, переданной пользователем.
7. Для всех трёх алгоритмов (решающее дерево, случайный лес, CatBoost) найдите индексы топ-5 самых важных признаков. В случае алгоритмов из `sklearn` используйте атрибут `feature_importances_`, для CatBoost можно использовать метод `get_feature_importance`.